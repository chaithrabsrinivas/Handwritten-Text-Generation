{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1972672,"sourceType":"datasetVersion","datasetId":1176843}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-01-23T06:42:11.334442Z","iopub.execute_input":"2024-01-23T06:42:11.334705Z","iopub.status.idle":"2024-01-23T06:42:14.842894Z","shell.execute_reply.started":"2024-01-23T06:42:11.33468Z","shell.execute_reply":"2024-01-23T06:42:14.841933Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Importing Libraries**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nfrom PIL import Image\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Reshape, Conv2D, BatchNormalization, UpSampling2D\n\nimport os\nimport shutil","metadata":{"execution":{"iopub.status.busy":"2024-01-23T06:42:14.844883Z","iopub.execute_input":"2024-01-23T06:42:14.845366Z","iopub.status.idle":"2024-01-23T06:42:26.44042Z","shell.execute_reply.started":"2024-01-23T06:42:14.845331Z","shell.execute_reply":"2024-01-23T06:42:26.439599Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Loading Data**","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv(\"/kaggle/input/english-handwritten-characters-dataset/english.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-01-23T06:42:26.441601Z","iopub.execute_input":"2024-01-23T06:42:26.442472Z","iopub.status.idle":"2024-01-23T06:42:26.463618Z","shell.execute_reply.started":"2024-01-23T06:42:26.442443Z","shell.execute_reply":"2024-01-23T06:42:26.46289Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.sample(2)","metadata":{"execution":{"iopub.status.busy":"2024-01-23T06:42:26.465874Z","iopub.execute_input":"2024-01-23T06:42:26.466171Z","iopub.status.idle":"2024-01-23T06:42:26.485653Z","shell.execute_reply.started":"2024-01-23T06:42:26.466147Z","shell.execute_reply":"2024-01-23T06:42:26.484633Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum().sum(),df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-01-23T06:42:26.486812Z","iopub.execute_input":"2024-01-23T06:42:26.487073Z","iopub.status.idle":"2024-01-23T06:42:26.500088Z","shell.execute_reply.started":"2024-01-23T06:42:26.48705Z","shell.execute_reply":"2024-01-23T06:42:26.499067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**No null or duplicate values**","metadata":{}},{"cell_type":"code","source":"df[\"label\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-23T06:42:26.501196Z","iopub.execute_input":"2024-01-23T06:42:26.501533Z","iopub.status.idle":"2024-01-23T06:42:26.511609Z","shell.execute_reply.started":"2024-01-23T06:42:26.501499Z","shell.execute_reply":"2024-01-23T06:42:26.510829Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Resizing images to 200x200**","metadata":{}},{"cell_type":"code","source":"images = df[\"image\"]\noutput=[]\nfor path in images:\n    \n    image = Image.open(\"/kaggle/input/english-handwritten-characters-dataset/\"+path)\n    image = image.convert(\"L\")\n    image = image.resize((500,500))\n    image = np.array(image)\n    output.append(image)\n\nimage_data = np.array(output) # Replace with your actual image data\nimage_data = image_data/255.0\n\nprint(image_data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-23T07:15:59.060152Z","iopub.execute_input":"2024-01-23T07:15:59.060524Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Ploting one image for check**","metadata":{}},{"cell_type":"code","source":"plt.gray()\nplt.imshow(image_data[200])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Encoding Input Text and converting to sequences**","metadata":{}},{"cell_type":"code","source":"text_data = df[\"label\"]\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True,lower=False)\ntokenizer.fit_on_texts(text_data)\n\n# Convert text data to sequences\ntext_sequences = tokenizer.texts_to_sequences(text_data)\n\n# Padding sequences for a consistent input size\nmax_text_length = max(len(seq) for seq in text_sequences)\npadded_text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences, maxlen=max_text_length)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(tokenizer.word_index)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Input Data contains total of 62 Character and Numbers**\n* **a-z : 26**\n* **A-Z : 26**\n* **0-9 : 10**","metadata":{}},{"cell_type":"markdown","source":"# Modelling\n\n* embedding layer for the input of LSTM(RNN)\n* LSTM layer for operating on text\n* Dense layer of size 400000 as output image is of size 200x200","metadata":{}},{"cell_type":"code","source":"model = models.Sequential()\nmodel.add(layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50))\nmodel.add(layers.LSTM(1024))\nmodel.add(layers.Dense(500*500, activation='relu'))\nmodel.add(layers.Reshape((500, 500, 1)))  # Assuming your output shape is 50x50\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\n# Train the model\nmodel.fit(np.array(padded_text_sequences), image_data, epochs=300, batch_size=64)","metadata":{"scrolled":true,"execution":{"iopub.status.idle":"2024-01-23T07:32:27.370429Z","shell.execute_reply.started":"2024-01-23T07:17:23.666101Z","shell.execute_reply":"2024-01-23T07:32:27.369554Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def delete_directory(directory_path):\n    try:\n        # Remove the contents of the directory\n        for root, dirs, files in os.walk(directory_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                os.remove(file_path)\n            for dir in dirs:\n                dir_path = os.path.join(root, dir)\n                os.rmdir(dir_path)\n\n        # Now, remove the empty directory\n        shutil.rmtree(directory_path)\n\n        print(f\"Directory '{directory_path}' already exists, '{directory_path}' deleted successfully.\")\n    except Exception as e:\n            print(\"does not exists\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-23T07:32:27.371533Z","iopub.execute_input":"2024-01-23T07:32:27.371868Z","iopub.status.idle":"2024-01-23T07:32:27.378282Z","shell.execute_reply.started":"2024-01-23T07:32:27.371842Z","shell.execute_reply":"2024-01-23T07:32:27.377381Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Creating Directory for storing Predicted Images**","metadata":{}},{"cell_type":"markdown","source":"**making white image if there is spaces in the input text**","metadata":{}},{"cell_type":"code","source":"white_image = np.ones((500, 500)) * 255\nfig,ax = plt.subplots(figsize = (1,1))\nplt.axis('off')\nplt.imshow(white_image,cmap='gray', vmin=0, vmax=255)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-23T07:32:44.966472Z","iopub.execute_input":"2024-01-23T07:32:44.966842Z","iopub.status.idle":"2024-01-23T07:32:45.036147Z","shell.execute_reply.started":"2024-01-23T07:32:44.966813Z","shell.execute_reply":"2024-01-23T07:32:45.034901Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Function for cropping and merging single images and making a full image**","metadata":{}},{"cell_type":"code","source":"def get_out_image(text=\"Sample\",figsize=(10,6),crop_size=(250,500)):\n    \n    mystr = text\n    textlist = list(mystr)\n\n    predicted_images=[]\n\n    for new_text in textlist:\n        if (new_text==\" \"):\n            white_image = np.ones((500, 500))*249\n            predicted_images.append(white_image)\n        else:\n            new_text_sequence = tokenizer.texts_to_sequences([new_text])\n            predicted_image = model.predict(np.array(new_text_sequence),verbose=5)[0]\n            predicted_image = predicted_image.reshape(500,500)\n            predicted_images.append(predicted_image)\n\n    directory_path = 'out_images'\n    delete_directory(directory_path)\n\n    import os\n    os.mkdir(\"out_images\")\n\n    predicted_images = np.array(predicted_images)\n\n    for new_text,image in zip(textlist,predicted_images):\n        image_path = f\"out_images/{new_text}.png\"\n        fig = plt.figure(figsize=(5,5))\n        if (new_text==\" \"):\n            fig.figimage(image,vmin=0,vmax=255)\n        else:\n            fig.figimage(image)\n        plt.savefig(image_path)\n        plt.close(fig)\n\n\n    def center_crop(image, target_size):\n        width, height = image.size\n        left = (width - target_size[0]) // 2\n        top = (height - target_size[1]) // 2\n        right = (width + target_size[0]) // 2\n        bottom = (height + target_size[1]) // 2\n        return image.crop((left, top, right, bottom))\n\n    def merge_images(image_list):\n        # Assuming all images have the same size\n        image_size = image_list[0].size\n        merged_image = np.zeros((image_size[1], len(image_list) * image_size[0]))\n\n        for i, image in enumerate(image_list):\n            merged_image[:, i * image_size[0] : (i + 1) * image_size[0]] = np.array(image)\n\n        return merged_image\n\n    # Load images and center crop\n    images = []\n    for char in textlist:\n        # Load image (replace 'path_to_images' with the actual path)\n        image_path = f\"out_images/{char}.png\"\n        img = Image.open(image_path).convert(\"L\")  # Convert to grayscale\n        cropped_img = center_crop(img, target_size=crop_size)  # Adjust the target size as needed\n        images.append(cropped_img)\n\n    # Merge images into a single numpy array\n    merged_image_array = merge_images(images)\n\n    # Display the thresholded merged image\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.imshow(merged_image_array, cmap=\"gray\",vmin=0, vmax=255)\n    ax.axis(\"off\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-23T07:50:28.135781Z","iopub.execute_input":"2024-01-23T07:50:28.136143Z","iopub.status.idle":"2024-01-23T07:50:28.151238Z","shell.execute_reply.started":"2024-01-23T07:50:28.136113Z","shell.execute_reply":"2024-01-23T07:50:28.150216Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Predicting Output**","metadata":{}},{"cell_type":"code","source":"get_out_image(\"Apple\",figsize=(2,3))","metadata":{"execution":{"iopub.status.busy":"2024-01-23T07:51:56.685606Z","iopub.execute_input":"2024-01-23T07:51:56.685988Z","iopub.status.idle":"2024-01-23T07:51:57.40829Z","shell.execute_reply.started":"2024-01-23T07:51:56.685958Z","shell.execute_reply":"2024-01-23T07:51:57.406777Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_out_image(\"Cat and Dog\",figsize=(4,5))","metadata":{"execution":{"iopub.status.busy":"2024-01-23T07:51:22.892536Z","iopub.execute_input":"2024-01-23T07:51:22.892905Z","iopub.status.idle":"2024-01-23T07:51:24.327381Z","shell.execute_reply.started":"2024-01-23T07:51:22.892875Z","shell.execute_reply":"2024-01-23T07:51:24.325946Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_out_image(\"Test Input Sample\",figsize=(4,5))","metadata":{"execution":{"iopub.status.busy":"2024-01-23T07:51:07.16645Z","iopub.execute_input":"2024-01-23T07:51:07.166816Z","iopub.status.idle":"2024-01-23T07:51:09.644308Z","shell.execute_reply.started":"2024-01-23T07:51:07.166786Z","shell.execute_reply":"2024-01-23T07:51:09.643281Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Conclusion**\n\n**Tried to apply the same model on full words dataset but loss was not reducing so i used this model**\n* **this dataset contains single characters photos data**\n* **i trained a model which takes one character as input and predicts single character output**\n* **so first the string is splitted into single characters and then it is sent to the model and model gives single photos as output,now i am merging all the output photos and showing the merged photo.**","metadata":{}}]}